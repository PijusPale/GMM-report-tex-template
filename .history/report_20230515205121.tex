\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage[utf8]{inputenc}
\usepackage[lithuanian]{babel}
\usepackage[T1]{fontenc}


% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[labelsep=endash]{caption}
\renewcommand{\figurename}{pav}
\renewcommand\IEEEkeywordsname{Raktažodžiai}

\usepackage{lipsum}  




\begin{document}

\title{Matematikos Sprendimas}

\author{\IEEEauthorblockN{Dominykas Dulevičius}
\IEEEauthorblockA{\textit{Informatikos institutas} \\
\textit{Matematikos ir informatikos fakultetas}\\
Vilnius, Lietuva \\
dominykas.dulevicius@mif.stud.vu.lt}
\and
\IEEEauthorblockN{Arnas Bulka}
\IEEEauthorblockA{\textit{Informatikos institutas} \\
\textit{Matematikos ir informatikos fakultetas}\\
Vilnius, Lietuva \\
arnas.bulka@mif.stud.vu.lt}
\and
\IEEEauthorblockN{Pijus Kizerskis}
\IEEEauthorblockA{\textit{Informatikos institutas} \\
\textit{Matematikos ir informatikos fakultetas}\\
Vilnius, Lietuva \\
pijus.kizerskis@mif.stud.vu.lt}
}

\maketitle

\begin{abstract}
Projektinio darbo metu tyrėme ir lyginome internete paplitusių populiarių modelių
galimybes ir tikslumą sprendžiant matematinius uždavinius bei lyginome rezultatus su
TP-Transformer tipo modeliu, kuris buvo specialiai aptreniruotas su DeepMind kūrėjų
„Mathematics-Dataset“ duomenų rinkiniu.
\end{abstract}

\begin{IEEEkeywords}
TP - Tenzoriaus-Produkto
\end{IEEEkeywords}


\section{Įvadas}
Matematinis samprotavimas – esminis žmogaus intelekto gebėjimas, veikiantis kaip pagrindinis mokslinės pažangos ir civilizacijos vystymosi jėga. Matematikos sprendimai nėra intuityvūs – mes uždavinius sprendžiame taikydami įvairias taisykles, interpretuodami simbolius bei taikydami globalias aksiomas. Pasauliui sparčiai technologiškai žengiant kartu su skaitmenitizacija ir dirbtinio intelekto įtaka kasdieniniams darbams ir įpročiams, mūsų darbo tikslas ištirti kaip efektyviai skirtingi modeliai ir jų architektūros sprendžia įvairaus lygio ir skirtingų temų matematinius uždavinius.


\section{Duomenų Rinkinys „Mathematics-Dataset“}


Mūsų pasirinktas duomenų rinkinys yra sudarytas tik iš laisvos formos sintetiškai sugeneruotų matematinių uždavinių iš skirtingų temų. Duomenų rinkinyje uždaviniai yra tik tekstiniai, neprašoma nupiešti grafikų ar medžių, nes tokius duomenis būtų daug sudėtingiau validuoti ir modeliai taptų kur kas sudėtingesni. Uždavinių generatorius sukurtas akcentuojant, kad uždaviniai būtų kuo aktualesni ir atspindėtų kasdienio gyvenimo „matematikos egzaminą“. Šis duomenų rinkinys buvo sukurtas ištirti bei pagerinti neuroninių tinklų modelių matematinio samprotavimo galimybes. Plačiau žr. „Analysing mathematical reasoning of neural models„~\cite{deeplab}. Straipsnyje lygino LSTM (angl. \textit{Long Short-Term Memory}), RMC (angl. \textit{Relational Memory Core}) ir Transformer tipų modelių variacijas. Geriausią tikslumą parodė TP-Transformer modelis kurį ir pasirinkome, kaip pagrindinį tyrimo objektą.

% \begin{equation}
% \mathcal{L}({\bf X}, {\bf Y}) = \frac{1}{w \cdot h} \sum_{i=1}^h \sum_{j=1}^w (X_{i, j} - Y_{i, j})^2
% % \label{eq:lygtis1}
% \end{equation}

% Taikyta nuostolių funkcija ~\eqref{eq:lygtis1}.

% \begin{align}
% y & = f(x) \nonumber \\
% f & = f_1(f_2(x))
% % \label{eq:lygtis2}
% \end{align}

% Taikytas modelis ~\eqref{eq:lygtis2}.


\section{transformer}

\subsection{Veikimas}
Populiariausias bei plačiausiai paplitęs natūralios kalbos modelis - transformeris. Jis pakeitė LSTM aplenkdamas pastarajį tiek efektyvesniu apmokymu, tiek tikslesniais bei greitesniais atsakymais. LSTM modelis tekstą apdoroja eidamas per kiekvieną žodį iš eilės bandydamas „prisiminti“ visą sakinio esmę. Transformeris apdoroja kiek įmanoma daugiau teksto vienu metu ir, atkreipdamas dėmesį į raktinius žodžius bando suprasti teksto esmę.

\begin{figure}
    \centering
    \includegraphics[scale=0.65]{transformer.png}
    \caption{Transformerio architektūra}
    \label{pav:1}
\end{figure}

\subsection{Moduliai}
Iš pirmo žvilgsnio modelio architektūra gali pasirodyti sudėtinga „~\ref{pav:1} pav.“, tačiau ją galima išskirti į dvi pagrindines dalis:
\begin{itemize}
  \item \textbf{Koduotuvas} (angl. \textit{Encoder}) yra atsakingas už įeities duomenų susistemizavimą bei užkodavimą. Tai modeliui padeda suvienodinti gaunamus duomenis bei juos paruošti kitam transformerio moduliui - dekoderiui.
  \item \textbf{Dekoderis} (angl. \textit{Decoder}) iš koduotuvo gauna galutinę paslėptą būseną pagal kurią sprendžia kiekvieno žodžio svarbumą. Susidėliojęs konteksto vektorių dekoderis atiduoda galutinę išvestį.
\end{itemize}
\subsection{Dėmesys}
Turbūt svarbiausias transformeryje naudojamas sluoksnis - susifokusavimas (angl. \textit{self-attention}). Būtent dėl šios savybės transfomeris geriau gali perprasti tekstą. Susifokusavimas figūruoja tiek koduotuvo, tiek dekoderio moduliuose padėdamas išgauti „paslėptas“ būsenas bei apskaičiuoti kiekvieno įvesto žodžio svarbumą. 

% \subsection{Equations}



% \subsection{Paveikslėliai ir lentelės}

% Paveikslėlį cituojame ``~\ref{fig} pav.''.

% Paveikslėlį cituojame ``~\ref{tab1} lentelė''.

% \begin{table}[htbp]
% \caption{Lentelės aprašas}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% a & b & c & sd \\
% \hline
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[!h] % įterpti čia
% \centerline{\includegraphics{fig1.png}}
% \caption{Paveikslėlio aprašas.}
% \label{fig}
% \end{figure}


\section{Naudojami Apmokyti Modeliai}
Lyginti su TP-Transformer pasirinkome 5 iš anksto apmokytus klausimų-atsakymų modelius: \textbf{
iAsk.Ai, DeepAI, Bing AI, CHAT GPT-4}. Šių modelių tikslumą matematikos uždavinių sprendimui lyginsime su \textbf{ischlag/TP-Transformer} modeliu,
specialiai pritaikytų matematikos uždaviniams spręsti.

\subsection{ischlag/TP-Transformer}
Šio modelio sukūrimui į standartinį transformerio tipo modelį buvo integruota Tenzoriaus-Produkto
reprezentacija, tokiu būdu pagerinant išskirtinius sąryšius tarp transformerio vidinių struktūrų. Modelis buvo apmokytas
naudojant „Mathematics-Dataset“, kuris buvo išskaidytas į 56 skirtingas matematikos šakas, siekiant
pagerinti modelio tikslumą bei atpažinamumą. Mūsų pasirinkta iš anksto ištreniruota modelio 
versija buvo treniruojama su 1.7 milijono duomenų.

\subsection{iAsk.Ai}
Šis kalbos variklis naudoja panašias technologijas, kaip ir ChatGPT variklis, tačiau
papildomas dėmesys yra skiriamas optimizuoti natūralios kalbos aprodorojimo modelį. iAsk AI taip pat susideda iš specialiai pritaikyto,
didelio masto Transformerio kalbos modelio. Šis modelis buvo išskirtinai mokomas remiantis
patikimiausiais ir autoritetiškiausiais literatūros bei interneto šaltiniais,
kas suteikia iAsk AI galimybę atsakyti į klausimus objektyviai,
faktiškai ir be potencialaus subjektyvumo, kurio galėtų būti ChatGPT.

\subsection{Bing AI}
BingAI - Microsoft kompanijos sukurtas dirbtinio intelekto variklis, naudojamas Bing paieškos sistemoje. Šis variklis naudoja tokias technologijas kaip gilųjį, mašininį mokymą ir natūralios kalbos apdorojimą, siekiant pagerinti paieškos rezultatus ir suteikti geriausius atsakymus vartotojams. Bing AI - nuolat tobulinamas ir mokomas naujų uždavinių, kad būtų užtikrinta patikima ir tiksli paieška. Taip pat Bing AI taikoma kitose Microsoft produktuose, tokiuose kaip Cortana ir Microsoft 365.

\subsection{DeepAI}
DeepAI - GPT-2 modeliu paremtas natūralios kalbos modelis. Šio, kaip ir kitų kalbos modelių, architektūroje slepiasi su dideliu kiekiu duomenų apmokytas transformeris.

\subsection{ChatGPT-4}
ChatGPT-4 yra paremtas GPT modeliu, kuris naudoja Transformer architektūros dekoderį. Transformer architektūra turi koduotuvą ir dekoderį, tačiau GPT naudoja tik autoregresine formos dekoderį, tai reiškia, kad jis yra optimizuotas tiksliai numatyti sekantį žodį sekoje. ChatGPT išskirtumas – duomenų rinkinio masyvumas. Jo duomenų rinkinys sudaro šimtai gigabaitų teksto.
\section{Tyrimo Eiga}
Iš „Mathematics-Dataset“ duomenų rinkinio 56 matematinių uždavinių kategorijų išsirinkome
po dvi klausimų-atsakymų poras. Iš viso gavosi duomenų rinkinys, su 112 klausimų ir 112 atsakymų.
Tada visus klausimus užduosime 4 pasirinktiems klausimų-atsakymų modeliams bei ischlag/TP-Transformer modeliui ir skaičiuosime
tikslumą - kiek iš užduotų klausimų modeliai sugebėjo atsakyti teisingai. \par
Toliau, padarėme kitą duomenų rinkinį iš 56 matematinių klausimų tokiu pačiu formatu ir iš tokių pačių temų,
tačiau šie klausimai jau nėra iš „Mathematics-Dataset“ duomenų rinkinio, o tai yra
atsitiktiniai klausimai. Šiems klausimams apsiskaičiavome atsakymus ir šiuos klausimus taip pat paduosime
visiems 6 modeliams ir žiūrėsime, kaip skirsis atsakymų tikslumas. Ypač įdomu turėtų būti stebėti
ischlag/TP-Transformer modelio tikslumą ant duomenų, kurių nebuvo jo treniruojamoje duomenų aibėje. \par
Klausimų ir atsakymų pavyzdžiai iš kiekvienos temos:
\subsection{Algebra}
\begin{itemize}
    \item Klausimas: Suppose -2*i + 20 = 5*o, 2*o - 4 = 3*i + o. Suppose -g + 2 = i, 4*g - 6*g = -5*u - 19. Let f be -6 + 3 + 2 + u/(-1). Solve -f*l = -l for l.
    \item Atsakymas: 0
\end{itemize}
\subsection{Aritmetika}
\begin{itemize}
    \item Klausimas: What is the tenth root of 95210445 to the nearest integer?
    \item Atsakymas: 6
\end{itemize}
\subsection{Matematinė Analizė}
\begin{itemize}
    \item Klausimas: Let s(w) be the third derivative of -7*w**2 - 1/30*w**5 - 23/6*w**3 + 0 + 0*w**6 + w - 1/42*w**7 + 1/24*w**4. What is the second derivative of s(l) wrt l?
    \item Atsakymas: -60*l**2 - 4
\end{itemize}
\subsection{Palyginimas}
\begin{itemize}
    \item Klausimas: Suppose -15*p + 48 + 27 = 0. Suppose 13 = 5*f - p*n - 2, 0 = 3*f - 2*n - 8. Let d be (-3 + 110/36)*f. Which is the third biggest value?  (a) 1/6  (b) d  (c) -4
    \item Atsakymas: c
\end{itemize}
\subsection{Konvertavimas}
\begin{itemize}
    \item Klausimas: How many micrometers are there in seven halves of a millimeter?
    \item Atsakymas: 3500
\end{itemize}
\subsection{Skaičių Operacijos}
\begin{itemize}
    \item Klausimas: Let a(s) = -154*s + 138. Let x be a(0). Let r(f) = -f - 3. Let z be r(-5). Suppose 4*v + u + u - 282 = 0, -2*v + z*u = -x. What are the prime factors of v?
    \item Atsakymas: 2, 5, 7
\end{itemize}
\subsection{Daugianariai}
\begin{itemize}
    \item Klausimas: Simplify ((y/y**(1/4)*y)**(3/47)*
    (y*y*y*y**(4/7)*y*y)/y**5)/(y/((y/(y*y/y**(-2/3)))/y*y*y)*
    (y*y/(y/(y**2/y)))/y*y**7/y**(4/3)) assuming y is positive.
    \item Atsakymas: y**(-30203/3948)
\end{itemize}
\subsection{Tikimybės}
\begin{itemize}
    \item Klausimas: Four letters picked without replacement from {a: 1, b: 1, s: 1, r: 1, q: 2, f: 2}. Give prob of sequence rbaq.
    \item Atsakymas: 1/840
\end{itemize}

Modelio spėjimai bus klasifikuojami kaip binariainiai: arba modelis atspėjo atsakymą, arba ne.
Atsakymas bus laikomas teisingu jei atsakymo reikškmė yra tokia pati, nepriklausomai nuo atsakymo duomenų formato.
Kaip vienas iš pavyzdžių galėtų būti: jeigu tikėtas atsakymas buvo 1.5, bet modelis grąžino
3/2, atsakymas buvo užskaitytas kaip teisingas.
Tikslumo metrika bus skaičiuojama padalinant teisingai atspėtų klausimų skaičių iš visų klausimų skaičiaus.

\section{Rezultatai}
Sudarėme dvi lenteles rezultatų analizėms: „~\ref{lentele:1} lentelė“ parodo, kiek klausimų iš 112 pateiktų
teisingai atspėjo modeliai iš „Mathematics-Dataset“ duomenų rinkinio. „~\ref{lentele:2} lentelė“  atvaizduoja,
kaip sekėsi tiems patiems modeliams spręsti atsitiktinius 56 klausimus iš tokių pačių matematinių temų.

\begin{table}[h!]
    \centering
    \caption{Klausimai iš „Mathematics-Dataset“.}
    \label{lentele:1}
    \begin{tabular}{|c c c|} 
     \hline
     Modelis & Teisingi atsakymai & Tikslumas \% \\ [0.5ex] 
     \hline\hline
     ischlag/TP-Transformer & 92 & 82.14 \\ 
     \hline
     Bing & 42 & 37.5 \\
     \hline
     iAsk.Ai & 41 & 36.61 \\
     \hline
     Chat GPT-4 & 30 & 26.78 \\
     \hline
     DeepAI & 27 & 24.11 \\
     \hline
    \end{tabular}
    \end{table}

\begin{table}[h!]
        \centering
        \caption{Atsitiktiniai Klausimai}
        \label{lentele:2}
        \begin{tabular}{|c c c|} 
        \hline
        Modelis & Teisingi atsakymai & Tikslumas \% \\ [0.5ex] 
        \hline\hline
        iAsk.Ai & 35 & 62.5 \\
        \hline
        Bing & 29 & 51.79 \\
        \hline
        DeepAI & 26 & 46.43 \\
        \hline
        ischlag/TP-Transformer & 20 & 36.71 \\
        \hline
        Chat GPT-4 & 18 & 32.14 \\
        \hline
    \end{tabular}
    \end{table}

\section{Išvados}
\begin{itemize}
    \item Geriausias modelis „Mathematics-Dataset“ duomenų rinkiniui yra ischlag/TP-Transformer su 82.15% tikslumu
    \item Geriausiai pasirodęs modelis su atsitiktiniais klausimais yra iAsk.Ai surinkęs 62.5%
    \item Galime matyti, kad atsitiktinius klausimus tiksliausiai sprendė iAsk.Ai modelis todėl, nes jo veikimo principas yra paremtas google paieškos tinkamiausiai aplikacijai spręsti užduotą uždavinį. Tą geriausiai iliustruoja, kad didžiąjai daliai klausimų iAsk.Ai naudojo WolframAlpha[šaltinis] matematikos variklį.
    \item Mažiausią tikslumą su atsitiktiniais klausimais surinko Chat GPT-4, nes, mūsų manymu, šis modelis yra labiau pritaikytas teksto generavimui, nei specifinių užduočių sprendimui.
    \item Iš ischlag/TP-Transformer tikslumo atsitiktiniams klausimams galime pastebėti, kad šis modelis gali gerai spręsti tik tuos klausimus kurie buvo „Mathematics-Dataset“ duomenų rinkinyje, nes būtent ant šių duomenų jis buvo ištreniruotas.
    \item Nesvarbu koks geras ir su kiek daug duomenų bus apmokytas modelis (pavyzdžiui ChatGPT-4), jeigu jis nebuvo treniruotas su specifinės srities duomenimis, jis gerai toje srityje nepasirodys.
\end{itemize}

\bibliographystyle{plain}
\bibliography{saltiniai}

\end{document}
